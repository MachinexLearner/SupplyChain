{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Evaluation & Accuracy Tracking\n",
    "\n",
    "**Purpose:** Systematically evaluate the Supply Chain Intelligence Agent across\n",
    "accuracy, hallucination, tool selection, response quality, and latency. Results\n",
    "are saved to a Delta table for tracking over time.\n",
    "\n",
    "**Depends on:** `03_supply_chain_agent_v2` (run that notebook first to initialize\n",
    "the agent, or this notebook bootstraps its own instance).\n",
    "\n",
    "### What it measures\n",
    "| Dimension | How |\n",
    "|-----------|-----|\n",
    "| **Tool Accuracy** | Did the agent call the correct tool(s)? |\n",
    "| **Hallucination** | Are numbers in the response traceable to tool output? |\n",
    "| **Relevance** | Does the response address the question asked? |\n",
    "| **Groundedness** | Does the response stay within tool-provided data? |\n",
    "| **Completeness** | Does the response include expected structural elements? |\n",
    "| **Latency** | How long did the agent take to respond? |\n",
    "\n",
    "### Output\n",
    "- Per-prompt scorecard displayed in-notebook\n",
    "- Aggregate summary with pass/fail rates\n",
    "- `supply_chain.gold.agent_evaluation_results` Delta table for trend analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%pip install --upgrade \"typing_extensions>=4.1\" \"langchain>=0.2,<0.4\" \"langchain-core>=0.2\" langgraph databricks-langchain mlflow pandas numpy\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "try:\n",
    "    from typing_extensions import Sentinel\n",
    "except ImportError:\n",
    "    dbutils.library.restartPython()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import time\n",
    "import re\n",
    "import json\n",
    "import hashlib\n",
    "from datetime import datetime, timezone\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Prompt Dictionary\n",
    "\n",
    "Each prompt specifies:\n",
    "- **expected_tools**: which tool(s) the agent should invoke\n",
    "- **required_keywords**: terms the response must contain\n",
    "- **banned_phrases**: terms that would indicate hallucination or error\n",
    "- **structural_checks**: callable validators for response quality\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "EVAL_PROMPTS = [\n",
    "    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "    # FORECASTING\n",
    "    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "    {\n",
    "        \"id\": \"FC-001\",\n",
    "        \"category\": \"Forecasting\",\n",
    "        \"prompt\": \"What is the demand forecast for the next 3 months?\",\n",
    "        \"expected_tools\": [\"get_demand_forecast\"],\n",
    "        \"required_keywords\": [\"forecast\", \"$\"],\n",
    "        \"banned_phrases\": [],\n",
    "        \"difficulty\": \"easy\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"FC-002\",\n",
    "        \"category\": \"Forecasting\",\n",
    "        \"prompt\": \"Compare the Prophet, ARIMA, and Random Forest forecasts for the next quarter. Which model should we rely on?\",\n",
    "        \"expected_tools\": [\"compare_forecast_models\"],\n",
    "        \"required_keywords\": [\"prophet\", \"arima\", \"random forest\"],\n",
    "        \"banned_phrases\": [],\n",
    "        \"difficulty\": \"medium\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"FC-003\",\n",
    "        \"category\": \"Forecasting\",\n",
    "        \"prompt\": \"How confident should we be in our demand forecasts for the next 6 months? What factors affect confidence?\",\n",
    "        \"expected_tools\": [\"assess_forecast_confidence\"],\n",
    "        \"required_keywords\": [\"confidence\", \"/100\"],\n",
    "        \"banned_phrases\": [],\n",
    "        \"difficulty\": \"medium\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"FC-004\",\n",
    "        \"category\": \"Forecasting\",\n",
    "        \"prompt\": \"Give me the 12-month demand forecast with confidence intervals.\",\n",
    "        \"expected_tools\": [\"get_demand_forecast\"],\n",
    "        \"required_keywords\": [\"$\", \"forecast\"],\n",
    "        \"banned_phrases\": [],\n",
    "        \"difficulty\": \"easy\",\n",
    "    },\n",
    "\n",
    "    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "    # ANALYSIS\n",
    "    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "    {\n",
    "        \"id\": \"AN-001\",\n",
    "        \"category\": \"Analysis\",\n",
    "        \"prompt\": \"Are there any demand anomalies in the last 6 months? Flag anything unusual.\",\n",
    "        \"expected_tools\": [\"detect_anomalies\"],\n",
    "        \"required_keywords\": [\"anomal\", \"baseline\"],\n",
    "        \"banned_phrases\": [],\n",
    "        \"difficulty\": \"easy\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"AN-002\",\n",
    "        \"category\": \"Analysis\",\n",
    "        \"prompt\": \"Analyze demand trends over the last 12 months. Is demand growing? Is there seasonality?\",\n",
    "        \"expected_tools\": [\"detect_trends\"],\n",
    "        \"required_keywords\": [\"trend\", \"growth\"],\n",
    "        \"banned_phrases\": [],\n",
    "        \"difficulty\": \"medium\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"AN-003\",\n",
    "        \"category\": \"Analysis\",\n",
    "        \"prompt\": \"What are the top factors driving our demand forecasts? Explain the key drivers.\",\n",
    "        \"expected_tools\": [\"explain_demand_drivers\"],\n",
    "        \"required_keywords\": [\"feature\", \"importance\"],\n",
    "        \"banned_phrases\": [],\n",
    "        \"difficulty\": \"medium\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"AN-004\",\n",
    "        \"category\": \"Analysis\",\n",
    "        \"prompt\": \"Detect anomalies with a 10% threshold over the last 12 months.\",\n",
    "        \"expected_tools\": [\"detect_anomalies\"],\n",
    "        \"required_keywords\": [\"10%\", \"anomal\"],\n",
    "        \"banned_phrases\": [],\n",
    "        \"difficulty\": \"easy\",\n",
    "    },\n",
    "\n",
    "    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "    # SCENARIOS\n",
    "    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "    {\n",
    "        \"id\": \"SC-001\",\n",
    "        \"category\": \"Scenarios\",\n",
    "        \"prompt\": \"What would happen to demand if geopolitical risk becomes CRITICAL in Europe?\",\n",
    "        \"expected_tools\": [\"scenario_geopolitical_risk\"],\n",
    "        \"required_keywords\": [\"critical\", \"$\"],\n",
    "        \"banned_phrases\": [],\n",
    "        \"difficulty\": \"easy\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"SC-002\",\n",
    "        \"category\": \"Scenarios\",\n",
    "        \"prompt\": \"Analyze the impact of a 50% tariff increase on steel imports.\",\n",
    "        \"expected_tools\": [\"scenario_tariff_increase\"],\n",
    "        \"required_keywords\": [\"tariff\", \"$\", \"50%\"],\n",
    "        \"banned_phrases\": [],\n",
    "        \"difficulty\": \"easy\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"SC-003\",\n",
    "        \"category\": \"Scenarios\",\n",
    "        \"prompt\": \"How would a major hurricane in the Gulf Coast affect our supply chain?\",\n",
    "        \"expected_tools\": [\"scenario_weather_disruption\"],\n",
    "        \"required_keywords\": [\"hurricane\", \"delay\"],\n",
    "        \"banned_phrases\": [],\n",
    "        \"difficulty\": \"easy\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"SC-004\",\n",
    "        \"category\": \"Scenarios\",\n",
    "        \"prompt\": \"Model a combined scenario: geopolitical risk up 75%, tariffs up 30%, and steel prices up 20%. What's the net impact?\",\n",
    "        \"expected_tools\": [\"build_whatif_scenario\"],\n",
    "        \"required_keywords\": [\"impact\", \"$\", \"%\"],\n",
    "        \"banned_phrases\": [],\n",
    "        \"difficulty\": \"hard\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"SC-005\",\n",
    "        \"category\": \"Scenarios\",\n",
    "        \"prompt\": \"What if there's a severe winter in the Midwest and geopolitical risk is elevated? Assess both.\",\n",
    "        \"expected_tools\": [\"scenario_weather_disruption\", \"scenario_geopolitical_risk\"],\n",
    "        \"required_keywords\": [\"winter\", \"geopolitical\"],\n",
    "        \"banned_phrases\": [],\n",
    "        \"difficulty\": \"hard\",\n",
    "    },\n",
    "\n",
    "    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "    # INTELLIGENCE\n",
    "    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "    {\n",
    "        \"id\": \"IN-001\",\n",
    "        \"category\": \"Intelligence\",\n",
    "        \"prompt\": \"Show me defense suppliers in Wisconsin.\",\n",
    "        \"expected_tools\": [\"query_suppliers\"],\n",
    "        \"required_keywords\": [\"supplier\", \"WI\"],\n",
    "        \"banned_phrases\": [],\n",
    "        \"difficulty\": \"easy\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"IN-002\",\n",
    "        \"category\": \"Intelligence\",\n",
    "        \"prompt\": \"Search for FPDS contracts with PSC code 23 from fiscal year 2024.\",\n",
    "        \"expected_tools\": [\"search_contracts\"],\n",
    "        \"required_keywords\": [\"contract\", \"23\"],\n",
    "        \"banned_phrases\": [],\n",
    "        \"difficulty\": \"easy\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"IN-003\",\n",
    "        \"category\": \"Intelligence\",\n",
    "        \"prompt\": \"How do our current metrics compare to DoD supply chain objectives?\",\n",
    "        \"expected_tools\": [\"compare_dod_metrics\"],\n",
    "        \"required_keywords\": [\"days of supply\", \"RO\"],\n",
    "        \"banned_phrases\": [],\n",
    "        \"difficulty\": \"medium\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"IN-004\",\n",
    "        \"category\": \"Intelligence\",\n",
    "        \"prompt\": \"What are the current prices for industrial metals and how might they affect our costs?\",\n",
    "        \"expected_tools\": [\"get_commodity_prices\"],\n",
    "        \"required_keywords\": [\"$\", \"price\"],\n",
    "        \"banned_phrases\": [],\n",
    "        \"difficulty\": \"medium\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"IN-005\",\n",
    "        \"category\": \"Intelligence\",\n",
    "        \"prompt\": \"What does the macroeconomic environment look like? Show me GSCPI and any trade indicators.\",\n",
    "        \"expected_tools\": [\"get_macro_context\"],\n",
    "        \"required_keywords\": [\"GSCPI\"],\n",
    "        \"banned_phrases\": [],\n",
    "        \"difficulty\": \"medium\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"IN-006\",\n",
    "        \"category\": \"Intelligence\",\n",
    "        \"prompt\": \"Find small business armor suppliers and list their locations.\",\n",
    "        \"expected_tools\": [\"query_suppliers\"],\n",
    "        \"required_keywords\": [\"supplier\", \"armor\"],\n",
    "        \"banned_phrases\": [],\n",
    "        \"difficulty\": \"medium\",\n",
    "    },\n",
    "\n",
    "    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "    # DASHBOARD / EXECUTIVE\n",
    "    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "    {\n",
    "        \"id\": \"EX-001\",\n",
    "        \"category\": \"Executive\",\n",
    "        \"prompt\": \"Generate a supply chain health dashboard.\",\n",
    "        \"expected_tools\": [\"get_supply_chain_health\"],\n",
    "        \"required_keywords\": [\"health\", \"/100\"],\n",
    "        \"banned_phrases\": [],\n",
    "        \"difficulty\": \"easy\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"EX-002\",\n",
    "        \"category\": \"Executive\",\n",
    "        \"prompt\": \"Give me a concise executive briefing on the current state of our supply chain.\",\n",
    "        \"expected_tools\": [\"generate_executive_briefing\"],\n",
    "        \"required_keywords\": [\"demand\", \"risk\", \"action\"],\n",
    "        \"banned_phrases\": [],\n",
    "        \"difficulty\": \"easy\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"EX-003\",\n",
    "        \"category\": \"Executive\",\n",
    "        \"prompt\": \"Brief the leadership team on demand outlook, risk environment, and recommended actions.\",\n",
    "        \"expected_tools\": [\"generate_executive_briefing\"],\n",
    "        \"required_keywords\": [\"outlook\", \"risk\", \"recommend\"],\n",
    "        \"banned_phrases\": [],\n",
    "        \"difficulty\": \"medium\",\n",
    "    },\n",
    "\n",
    "    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "    # MULTI-TOOL / COMPLEX\n",
    "    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "    {\n",
    "        \"id\": \"MX-001\",\n",
    "        \"category\": \"Multi-Tool\",\n",
    "        \"prompt\": \"I need a full picture: current forecast, confidence level, and any anomalies. Summarize it all.\",\n",
    "        \"expected_tools\": [\"get_demand_forecast\", \"assess_forecast_confidence\", \"detect_anomalies\"],\n",
    "        \"required_keywords\": [\"forecast\", \"confidence\", \"anomal\"],\n",
    "        \"banned_phrases\": [],\n",
    "        \"difficulty\": \"hard\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"MX-002\",\n",
    "        \"category\": \"Multi-Tool\",\n",
    "        \"prompt\": \"How healthy is our supply chain? Show me the health score and the DoD metrics side by side.\",\n",
    "        \"expected_tools\": [\"get_supply_chain_health\", \"compare_dod_metrics\"],\n",
    "        \"required_keywords\": [\"health\", \"days of supply\"],\n",
    "        \"banned_phrases\": [],\n",
    "        \"difficulty\": \"hard\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"MX-003\",\n",
    "        \"category\": \"Multi-Tool\",\n",
    "        \"prompt\": \"What are commodity price trends and how do they correlate with our demand patterns?\",\n",
    "        \"expected_tools\": [\"get_commodity_prices\", \"detect_trends\"],\n",
    "        \"required_keywords\": [\"commodity\", \"trend\"],\n",
    "        \"banned_phrases\": [],\n",
    "        \"difficulty\": \"hard\",\n",
    "    },\n",
    "\n",
    "    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "    # HALLUCINATION TRAPS \u2014 prompts designed to catch fabrication\n",
    "    # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "    {\n",
    "        \"id\": \"HT-001\",\n",
    "        \"category\": \"Hallucination Trap\",\n",
    "        \"prompt\": \"What was our exact demand on March 15, 2024?\",\n",
    "        \"expected_tools\": [],\n",
    "        \"required_keywords\": [],\n",
    "        \"banned_phrases\": [\"$1\", \"$2\", \"$3\", \"$4\", \"$5\", \"$6\", \"$7\", \"$8\", \"$9\"],\n",
    "        \"difficulty\": \"hard\",\n",
    "        \"notes\": \"Agent should NOT fabricate a specific daily number \u2014 our data is monthly.\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"HT-002\",\n",
    "        \"category\": \"Hallucination Trap\",\n",
    "        \"prompt\": \"What is the name of the CEO of our biggest supplier?\",\n",
    "        \"expected_tools\": [\"query_suppliers\"],\n",
    "        \"required_keywords\": [],\n",
    "        \"banned_phrases\": [],\n",
    "        \"difficulty\": \"hard\",\n",
    "        \"notes\": \"Agent should admit it doesn't have CEO names in the data rather than inventing one.\",\n",
    "        \"hallucination_check\": \"should_not_name_person\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"HT-003\",\n",
    "        \"category\": \"Hallucination Trap\",\n",
    "        \"prompt\": \"Tell me about our operations in Germany and Japan.\",\n",
    "        \"expected_tools\": [],\n",
    "        \"required_keywords\": [],\n",
    "        \"banned_phrases\": [],\n",
    "        \"difficulty\": \"hard\",\n",
    "        \"notes\": \"Agent should clarify it doesn't have data on specific foreign operations unless they appear in supplier data.\",\n",
    "        \"hallucination_check\": \"should_express_uncertainty\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Evaluation dictionary: {len(EVAL_PROMPTS)} prompts across {len(set(p['category'] for p in EVAL_PROMPTS))} categories\")\n",
    "for cat in sorted(set(p['category'] for p in EVAL_PROMPTS)):\n",
    "    count = sum(1 for p in EVAL_PROMPTS if p['category'] == cat)\n",
    "    print(f\"  {cat}: {count} prompts\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Framework\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "@dataclass\n",
    "class EvalResult:\n",
    "    \"\"\"Result of evaluating a single prompt.\"\"\"\n",
    "    prompt_id: str\n",
    "    category: str\n",
    "    difficulty: str\n",
    "    prompt: str\n",
    "    response: str = \"\"\n",
    "    \n",
    "    # Tool selection\n",
    "    expected_tools: list = field(default_factory=list)\n",
    "    actual_tools: list = field(default_factory=list)\n",
    "    tool_accuracy: float = 0.0            # 0-1: fraction of expected tools called\n",
    "    unexpected_tools: list = field(default_factory=list)\n",
    "    \n",
    "    # Content quality\n",
    "    keyword_hits: int = 0\n",
    "    keyword_misses: int = 0\n",
    "    keyword_score: float = 0.0            # 0-1: fraction of required keywords found\n",
    "    banned_phrase_violations: int = 0\n",
    "    \n",
    "    # Hallucination\n",
    "    hallucination_score: float = 0.0      # 0-1: 0 = no hallucination, 1 = definite hallucination\n",
    "    hallucination_flags: list = field(default_factory=list)\n",
    "    numbers_in_response: int = 0\n",
    "    numbers_traceable: int = 0\n",
    "    \n",
    "    # Groundedness (does response use tool output?)\n",
    "    groundedness_score: float = 0.0       # 0-1: 1 = fully grounded\n",
    "    \n",
    "    # Meta\n",
    "    latency_seconds: float = 0.0\n",
    "    error: str = \"\"\n",
    "    passed: bool = False\n",
    "    overall_score: float = 0.0            # 0-100 composite\n",
    "    \n",
    "    timestamp: str = \"\"\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instrumented Agent Runner\n",
    "\n",
    "Wraps the agent to capture tool calls and tool outputs for evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class InstrumentedAgent:\n",
    "    \"\"\"\n",
    "    Wraps the agent executor to capture tool invocations and their outputs\n",
    "    for evaluation purposes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm, tools, system_prompt):\n",
    "        from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, SystemMessage\n",
    "        self.llm = llm\n",
    "        self.tools = tools\n",
    "        self.tools_by_name = {t.name: t for t in tools}\n",
    "        self.system_prompt = system_prompt\n",
    "        self.HumanMessage = HumanMessage\n",
    "        self.SystemMessage = SystemMessage\n",
    "        self.ToolMessage = ToolMessage\n",
    "    \n",
    "    def run(self, prompt: str) -> dict:\n",
    "        \"\"\"\n",
    "        Run a prompt and return structured results including tool trace.\n",
    "        \n",
    "        Returns:\n",
    "            {\n",
    "                \"output\": str,\n",
    "                \"tools_called\": [{\"name\": str, \"args\": dict, \"output\": str}],\n",
    "                \"latency\": float,\n",
    "                \"error\": str | None,\n",
    "            }\n",
    "        \"\"\"\n",
    "        tools_called = []\n",
    "        \n",
    "        messages = [\n",
    "            self.SystemMessage(content=self.system_prompt),\n",
    "            self.HumanMessage(content=prompt),\n",
    "        ]\n",
    "        \n",
    "        start = time.time()\n",
    "        error = None\n",
    "        final_output = \"\"\n",
    "        \n",
    "        try:\n",
    "            max_rounds = 10\n",
    "            for _ in range(max_rounds):\n",
    "                response = self.llm.bind_tools(self.tools).invoke(messages)\n",
    "                \n",
    "                if not getattr(response, \"tool_calls\", None):\n",
    "                    final_output = response.content or \"\"\n",
    "                    break\n",
    "                \n",
    "                messages.append(response)\n",
    "                \n",
    "                for tc in response.tool_calls:\n",
    "                    name = tc.get(\"name\") if isinstance(tc, dict) else getattr(tc, \"name\", None)\n",
    "                    args = tc.get(\"args\", {}) if isinstance(tc, dict) else getattr(tc, \"args\", {})\n",
    "                    tid = tc.get(\"id\", \"\") if isinstance(tc, dict) else getattr(tc, \"id\", \"\")\n",
    "                    \n",
    "                    tool_fn = self.tools_by_name.get(name)\n",
    "                    tool_output = \"\"\n",
    "                    if tool_fn:\n",
    "                        try:\n",
    "                            tool_output = str(tool_fn.invoke(args))\n",
    "                        except Exception as te:\n",
    "                            tool_output = f\"TOOL_ERROR: {te}\"\n",
    "                    \n",
    "                    tools_called.append({\n",
    "                        \"name\": name,\n",
    "                        \"args\": args,\n",
    "                        \"output\": tool_output,\n",
    "                    })\n",
    "                    \n",
    "                    messages.append(self.ToolMessage(content=tool_output, tool_call_id=tid))\n",
    "            else:\n",
    "                final_output = (response.content or \"\") + \" [Max rounds reached]\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            error = str(e)\n",
    "            final_output = f\"AGENT_ERROR: {e}\"\n",
    "        \n",
    "        latency = time.time() - start\n",
    "        \n",
    "        return {\n",
    "            \"output\": final_output,\n",
    "            \"tools_called\": tools_called,\n",
    "            \"latency\": latency,\n",
    "            \"error\": error,\n",
    "        }\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validators\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def check_tool_accuracy(expected: list, actual_calls: list) -> tuple:\n",
    "    \"\"\"\n",
    "    Check if the expected tools were called.\n",
    "    Returns (accuracy 0-1, list of unexpected tools).\n",
    "    \"\"\"\n",
    "    actual_names = [tc[\"name\"] for tc in actual_calls]\n",
    "    \n",
    "    if not expected:\n",
    "        # No specific tool expected \u2014 any tool call is fine (or none)\n",
    "        return 1.0, []\n",
    "    \n",
    "    hits = sum(1 for t in expected if t in actual_names)\n",
    "    accuracy = hits / len(expected)\n",
    "    unexpected = [t for t in actual_names if t not in expected]\n",
    "    \n",
    "    return accuracy, unexpected\n",
    "\n",
    "\n",
    "def check_keywords(response: str, required_keywords: list) -> tuple:\n",
    "    \"\"\"\n",
    "    Check if required keywords appear in the response (case-insensitive).\n",
    "    Returns (hits, misses, score 0-1).\n",
    "    \"\"\"\n",
    "    if not required_keywords:\n",
    "        return 0, 0, 1.0\n",
    "    \n",
    "    response_lower = response.lower()\n",
    "    hits = sum(1 for kw in required_keywords if kw.lower() in response_lower)\n",
    "    misses = len(required_keywords) - hits\n",
    "    score = hits / len(required_keywords)\n",
    "    \n",
    "    return hits, misses, score\n",
    "\n",
    "\n",
    "def check_banned_phrases(response: str, banned: list) -> int:\n",
    "    \"\"\"Count how many banned phrases appear in the response.\"\"\"\n",
    "    if not banned:\n",
    "        return 0\n",
    "    response_lower = response.lower()\n",
    "    return sum(1 for phrase in banned if phrase.lower() in response_lower)\n",
    "\n",
    "\n",
    "def check_hallucination(response: str, tool_outputs: list, prompt_config: dict) -> tuple:\n",
    "    \"\"\"\n",
    "    Detect potential hallucinations by checking if numbers in the response\n",
    "    can be traced back to tool outputs.\n",
    "    \n",
    "    Returns (hallucination_score 0-1, flags list, n_numbers, n_traceable).\n",
    "    \"\"\"\n",
    "    flags = []\n",
    "    \n",
    "    # Extract dollar amounts and large numbers from response\n",
    "    # Match patterns like $1,234,567 or $1.2M or plain numbers > 999\n",
    "    dollar_pattern = r'\\$[\\d,]+(?:\\.\\d+)?'\n",
    "    number_pattern = r'\\b\\d{4,}\\b'\n",
    "    \n",
    "    response_numbers = set()\n",
    "    for match in re.findall(dollar_pattern, response):\n",
    "        cleaned = match.replace('$', '').replace(',', '')\n",
    "        try:\n",
    "            response_numbers.add(float(cleaned))\n",
    "        except ValueError:\n",
    "            pass\n",
    "    for match in re.findall(number_pattern, response):\n",
    "        try:\n",
    "            val = float(match)\n",
    "            if val > 1900 and val < 2100:\n",
    "                continue  # Skip years\n",
    "            response_numbers.add(val)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    if not response_numbers:\n",
    "        return 0.0, [], 0, 0\n",
    "    \n",
    "    # Collect all numbers from tool outputs\n",
    "    tool_number_text = \" \".join(tool_outputs)\n",
    "    tool_numbers = set()\n",
    "    for match in re.findall(dollar_pattern, tool_number_text):\n",
    "        cleaned = match.replace('$', '').replace(',', '')\n",
    "        try:\n",
    "            tool_numbers.add(float(cleaned))\n",
    "        except ValueError:\n",
    "            pass\n",
    "    for match in re.findall(number_pattern, tool_number_text):\n",
    "        try:\n",
    "            val = float(match)\n",
    "            tool_numbers.add(val)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    # Check traceability \u2014 a number is \"traceable\" if it appears in tool output\n",
    "    # or is a reasonable derivation (within 5% of a tool number)\n",
    "    traceable = 0\n",
    "    for num in response_numbers:\n",
    "        is_traceable = False\n",
    "        for tool_num in tool_numbers:\n",
    "            if tool_num == 0:\n",
    "                if num == 0:\n",
    "                    is_traceable = True\n",
    "                    break\n",
    "            elif abs(num - tool_num) / abs(tool_num) < 0.05:\n",
    "                is_traceable = True\n",
    "                break\n",
    "        if is_traceable:\n",
    "            traceable += 1\n",
    "        else:\n",
    "            flags.append(f\"Untraceable number: {num}\")\n",
    "    \n",
    "    n_total = len(response_numbers)\n",
    "    n_traceable = traceable\n",
    "    \n",
    "    # Special hallucination checks\n",
    "    special = prompt_config.get(\"hallucination_check\", \"\")\n",
    "    if special == \"should_not_name_person\":\n",
    "        # Check if the response contains what looks like a person's name after \"CEO\" or \"president\"\n",
    "        name_after_title = re.search(r'(?:CEO|president|chief)\\s+(?:is\\s+)?([A-Z][a-z]+\\s+[A-Z][a-z]+)', response)\n",
    "        if name_after_title:\n",
    "            flags.append(f\"Fabricated person name: {name_after_title.group(1)}\")\n",
    "    \n",
    "    if special == \"should_express_uncertainty\":\n",
    "        uncertainty_words = [\"don't have\", \"not available\", \"no data\", \"cannot confirm\",\n",
    "                           \"uncertain\", \"unable to\", \"not in\", \"no information\"]\n",
    "        if not any(w in response.lower() for w in uncertainty_words):\n",
    "            flags.append(\"Did not express uncertainty when expected\")\n",
    "    \n",
    "    # Score: 0 = perfect (no hallucination), 1 = all numbers untraceable\n",
    "    if n_total > 0:\n",
    "        hallucination_score = 1.0 - (n_traceable / n_total)\n",
    "    else:\n",
    "        hallucination_score = 0.0\n",
    "    \n",
    "    # Special checks can override\n",
    "    if flags and hallucination_score == 0:\n",
    "        hallucination_score = 0.5\n",
    "    \n",
    "    return hallucination_score, flags, n_total, n_traceable\n",
    "\n",
    "\n",
    "def check_groundedness(response: str, tool_outputs: list) -> float:\n",
    "    \"\"\"\n",
    "    Check how grounded the response is in tool output.\n",
    "    Uses simple overlap: what fraction of response sentences contain\n",
    "    information from tool output.\n",
    "    \"\"\"\n",
    "    if not tool_outputs or not response:\n",
    "        return 0.5  # Neutral if no tool output\n",
    "    \n",
    "    combined_tools = \" \".join(tool_outputs).lower()\n",
    "    sentences = [s.strip() for s in re.split(r'[.!?\\n]', response) if len(s.strip()) > 20]\n",
    "    \n",
    "    if not sentences:\n",
    "        return 0.5\n",
    "    \n",
    "    grounded = 0\n",
    "    for sent in sentences:\n",
    "        # Check if key terms from the sentence appear in tool output\n",
    "        words = set(re.findall(r'\\b\\w{4,}\\b', sent.lower()))\n",
    "        if not words:\n",
    "            continue\n",
    "        overlap = sum(1 for w in words if w in combined_tools)\n",
    "        if overlap / len(words) > 0.3:\n",
    "            grounded += 1\n",
    "    \n",
    "    return grounded / len(sentences)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Agent for Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# \u2500\u2500 Import agent components from 03_supply_chain_agent_v2 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# We re-create the agent here to get the instrumented version.\n",
    "\n",
    "from databricks_langchain import ChatDatabricks\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, SystemMessage\n",
    "\n",
    "CATALOG = \"supply_chain\"\n",
    "TABLES = {\n",
    "    \"demand_signals\":       f\"{CATALOG}.gold.oshkosh_monthly_demand_signals\",\n",
    "    \"dod_metrics\":          f\"{CATALOG}.gold.dod_metrics_inputs_monthly\",\n",
    "    \"trade_risk\":           f\"{CATALOG}.gold.trade_tariff_risk_monthly\",\n",
    "    \"prophet_forecasts\":    f\"{CATALOG}.gold.prophet_forecasts\",\n",
    "    \"arima_forecasts\":      f\"{CATALOG}.gold.arima_forecasts\",\n",
    "    \"rf_forecasts\":         f\"{CATALOG}.gold.random_forest_forecasts\",\n",
    "    \"rf_feature_importance\": f\"{CATALOG}.gold.random_forest_feature_importance\",\n",
    "    \"suppliers\":            f\"{CATALOG}.silver.supplier_geolocations\",\n",
    "    \"commodity\":            f\"{CATALOG}.silver.commodity_prices_monthly\",\n",
    "    \"weather\":              f\"{CATALOG}.silver.weather_risk_monthly\",\n",
    "    \"fpds_contracts\":       f\"{CATALOG}.bronze.fpds_contracts\",\n",
    "    \"gscpi\":                f\"{CATALOG}.bronze.nyfed_gscpi\",\n",
    "    \"wto\":                  f\"{CATALOG}.bronze.wto_trade_barometer\",\n",
    "}\n",
    "\n",
    "\n",
    "def _safe_load(table_key):\n",
    "    table_name = TABLES.get(table_key, table_key)\n",
    "    try:\n",
    "        return spark.table(table_name).toPandas()\n",
    "    except Exception:\n",
    "        return pd.DataFrame()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-register tools (same as agent notebook \u2014 needed for instrumented runner)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# %run ./03_supply_chain_agent_v2\n",
    "# If the above %run doesn't work (e.g. tools aren't importable), the eval\n",
    "# notebook re-defines the tools inline.  We import them by running the agent\n",
    "# notebook.  Uncomment the %run above if you prefer that approach.\n",
    "\n",
    "# For a standalone run, we import the tool definitions directly.\n",
    "# This cell mirrors the tool registration from 03_supply_chain_agent_v2.\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# We re-use _safe_load defined above.  The tool bodies are identical to the\n",
    "# agent notebook \u2014 see 03_supply_chain_agent_v2 for full implementations.\n",
    "# Here we use a compact re-export via %run.\n",
    "\n",
    "%run ./03_supply_chain_agent_v2\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Instrumented Runner\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "SYSTEM_PROMPT_EVAL = SYSTEM_PROMPT  # imported from %run above\n",
    "\n",
    "eval_llm = ChatDatabricks(\n",
    "    endpoint=\"databricks-meta-llama-3-3-70b-instruct\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=2000,\n",
    ")\n",
    "\n",
    "instrumented_agent = InstrumentedAgent(eval_llm, all_tools, SYSTEM_PROMPT_EVAL)\n",
    "print(f\"Instrumented agent ready with {len(all_tools)} tools\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dbutils.widgets.dropdown(\"eval_scope\", \"all\", [\"all\", \"easy\", \"medium\", \"hard\", \"forecasting\", \"analysis\", \"scenarios\", \"intelligence\", \"executive\", \"multi-tool\", \"hallucination\"], \"Evaluation scope\")\n",
    "dbutils.widgets.text(\"max_prompts\", \"0\", \"Max prompts (0 = all)\")\n",
    "\n",
    "scope = dbutils.widgets.get(\"eval_scope\").lower()\n",
    "max_prompts = int(dbutils.widgets.get(\"max_prompts\"))\n",
    "\n",
    "# Filter prompts\n",
    "if scope == \"all\":\n",
    "    prompts_to_run = EVAL_PROMPTS\n",
    "elif scope in (\"easy\", \"medium\", \"hard\"):\n",
    "    prompts_to_run = [p for p in EVAL_PROMPTS if p[\"difficulty\"] == scope]\n",
    "else:\n",
    "    prompts_to_run = [p for p in EVAL_PROMPTS if p[\"category\"].lower().replace(\" \", \"-\").replace(\"_\", \"-\") == scope.replace(\" \", \"-\").replace(\"_\", \"-\")]\n",
    "\n",
    "if max_prompts > 0:\n",
    "    prompts_to_run = prompts_to_run[:max_prompts]\n",
    "\n",
    "print(f\"Running {len(prompts_to_run)} prompts (scope={scope})\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "results = []\n",
    "run_id = hashlib.md5(datetime.now(timezone.utc).isoformat().encode()).hexdigest()[:12]\n",
    "run_timestamp = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "for i, prompt_cfg in enumerate(prompts_to_run):\n",
    "    pid = prompt_cfg[\"id\"]\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"[{i+1}/{len(prompts_to_run)}] {pid}: {prompt_cfg['prompt'][:80]}...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Run agent\n",
    "    agent_result = instrumented_agent.run(prompt_cfg[\"prompt\"])\n",
    "    \n",
    "    response = agent_result[\"output\"]\n",
    "    tools_called = agent_result[\"tools_called\"]\n",
    "    tool_outputs = [tc[\"output\"] for tc in tools_called]\n",
    "    \n",
    "    # \u2500\u2500 Evaluate \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    tool_acc, unexpected = check_tool_accuracy(\n",
    "        prompt_cfg[\"expected_tools\"], tools_called\n",
    "    )\n",
    "    \n",
    "    kw_hits, kw_misses, kw_score = check_keywords(\n",
    "        response, prompt_cfg.get(\"required_keywords\", [])\n",
    "    )\n",
    "    \n",
    "    banned_violations = check_banned_phrases(\n",
    "        response, prompt_cfg.get(\"banned_phrases\", [])\n",
    "    )\n",
    "    \n",
    "    hall_score, hall_flags, n_nums, n_trace = check_hallucination(\n",
    "        response, tool_outputs, prompt_cfg\n",
    "    )\n",
    "    \n",
    "    grounded = check_groundedness(response, tool_outputs)\n",
    "    \n",
    "    # \u2500\u2500 Composite score \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    # Weights: tool accuracy 25%, keywords 20%, hallucination 30%, groundedness 15%, no-banned 10%\n",
    "    banned_score = 1.0 if banned_violations == 0 else 0.0\n",
    "    composite = (\n",
    "        tool_acc * 25 +\n",
    "        kw_score * 20 +\n",
    "        (1.0 - hall_score) * 30 +\n",
    "        grounded * 15 +\n",
    "        banned_score * 10\n",
    "    )\n",
    "    \n",
    "    passed = composite >= 60 and hall_score < 0.5\n",
    "    \n",
    "    result = EvalResult(\n",
    "        prompt_id=pid,\n",
    "        category=prompt_cfg[\"category\"],\n",
    "        difficulty=prompt_cfg[\"difficulty\"],\n",
    "        prompt=prompt_cfg[\"prompt\"],\n",
    "        response=response[:2000],\n",
    "        expected_tools=prompt_cfg[\"expected_tools\"],\n",
    "        actual_tools=[tc[\"name\"] for tc in tools_called],\n",
    "        tool_accuracy=tool_acc,\n",
    "        unexpected_tools=unexpected,\n",
    "        keyword_hits=kw_hits,\n",
    "        keyword_misses=kw_misses,\n",
    "        keyword_score=kw_score,\n",
    "        banned_phrase_violations=banned_violations,\n",
    "        hallucination_score=hall_score,\n",
    "        hallucination_flags=hall_flags,\n",
    "        numbers_in_response=n_nums,\n",
    "        numbers_traceable=n_trace,\n",
    "        groundedness_score=grounded,\n",
    "        latency_seconds=agent_result[\"latency\"],\n",
    "        error=agent_result.get(\"error\", \"\") or \"\",\n",
    "        passed=passed,\n",
    "        overall_score=composite,\n",
    "        timestamp=run_timestamp,\n",
    "    )\n",
    "    \n",
    "    results.append(result)\n",
    "    \n",
    "    # Print summary\n",
    "    status = \"PASS\" if passed else \"FAIL\"\n",
    "    print(f\"  Status: {status}  |  Score: {composite:.0f}/100  |  Latency: {agent_result['latency']:.1f}s\")\n",
    "    print(f\"  Tools: expected={prompt_cfg['expected_tools']} actual={[tc['name'] for tc in tools_called]}\")\n",
    "    print(f\"  Tool Acc: {tool_acc:.0%}  |  Keywords: {kw_score:.0%}  |  Hallucination: {hall_score:.0%}  |  Grounded: {grounded:.0%}\")\n",
    "    if hall_flags:\n",
    "        print(f\"  Hallucination flags: {hall_flags}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Build results DataFrame\n",
    "results_data = []\n",
    "for r in results:\n",
    "    results_data.append({\n",
    "        \"run_id\": run_id,\n",
    "        \"prompt_id\": r.prompt_id,\n",
    "        \"category\": r.category,\n",
    "        \"difficulty\": r.difficulty,\n",
    "        \"prompt\": r.prompt,\n",
    "        \"response_preview\": r.response[:500],\n",
    "        \"expected_tools\": json.dumps(r.expected_tools),\n",
    "        \"actual_tools\": json.dumps(r.actual_tools),\n",
    "        \"tool_accuracy\": r.tool_accuracy,\n",
    "        \"keyword_score\": r.keyword_score,\n",
    "        \"hallucination_score\": r.hallucination_score,\n",
    "        \"hallucination_flags\": json.dumps(r.hallucination_flags),\n",
    "        \"groundedness_score\": r.groundedness_score,\n",
    "        \"overall_score\": r.overall_score,\n",
    "        \"passed\": r.passed,\n",
    "        \"latency_seconds\": r.latency_seconds,\n",
    "        \"error\": r.error,\n",
    "        \"timestamp\": r.timestamp,\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# \u2500\u2500 Aggregate Summary \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "n_total = len(results)\n",
    "n_passed = sum(1 for r in results if r.passed)\n",
    "n_failed = n_total - n_passed\n",
    "avg_score = np.mean([r.overall_score for r in results])\n",
    "avg_latency = np.mean([r.latency_seconds for r in results])\n",
    "avg_tool_acc = np.mean([r.tool_accuracy for r in results])\n",
    "avg_kw = np.mean([r.keyword_score for r in results])\n",
    "avg_hall = np.mean([r.hallucination_score for r in results])\n",
    "avg_grounded = np.mean([r.groundedness_score for r in results])\n",
    "\n",
    "print(f\"\"\"\n",
    "{'='*60}\n",
    "  AGENT EVALUATION SUMMARY\n",
    "  Run ID: {run_id}  |  {run_timestamp}\n",
    "{'='*60}\n",
    "\n",
    "  Prompts Tested:    {n_total}\n",
    "  Passed:            {n_passed} ({n_passed/n_total*100:.0f}%)\n",
    "  Failed:            {n_failed} ({n_failed/n_total*100:.0f}%)\n",
    "  \n",
    "  AGGREGATE SCORES (averages):\n",
    "  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "  Overall Score:     {avg_score:.1f}/100\n",
    "  Tool Accuracy:     {avg_tool_acc:.0%}\n",
    "  Keyword Relevance: {avg_kw:.0%}\n",
    "  Hallucination:     {avg_hall:.0%}  (lower is better)\n",
    "  Groundedness:      {avg_grounded:.0%}\n",
    "  Avg Latency:       {avg_latency:.1f}s\n",
    "\"\"\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# \u2500\u2500 By Category \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "print(\"SCORES BY CATEGORY:\")\n",
    "print(f\"{'Category':<20s} {'N':>3s} {'Pass%':>6s} {'Score':>6s} {'ToolAcc':>8s} {'Halluc':>7s} {'Latency':>8s}\")\n",
    "print(\"\u2500\" * 62)\n",
    "for cat in sorted(set(r.category for r in results)):\n",
    "    cat_results = [r for r in results if r.category == cat]\n",
    "    n = len(cat_results)\n",
    "    pass_pct = sum(1 for r in cat_results if r.passed) / n * 100\n",
    "    avg_sc = np.mean([r.overall_score for r in cat_results])\n",
    "    avg_ta = np.mean([r.tool_accuracy for r in cat_results])\n",
    "    avg_h = np.mean([r.hallucination_score for r in cat_results])\n",
    "    avg_l = np.mean([r.latency_seconds for r in cat_results])\n",
    "    print(f\"{cat:<20s} {n:>3d} {pass_pct:>5.0f}% {avg_sc:>5.1f} {avg_ta:>7.0%} {avg_h:>6.0%} {avg_l:>7.1f}s\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# \u2500\u2500 By Difficulty \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "print(\"\\nSCORES BY DIFFICULTY:\")\n",
    "print(f\"{'Difficulty':<12s} {'N':>3s} {'Pass%':>6s} {'Score':>6s} {'Halluc':>7s}\")\n",
    "print(\"\u2500\" * 38)\n",
    "for diff in [\"easy\", \"medium\", \"hard\"]:\n",
    "    diff_results = [r for r in results if r.difficulty == diff]\n",
    "    if diff_results:\n",
    "        n = len(diff_results)\n",
    "        pass_pct = sum(1 for r in diff_results if r.passed) / n * 100\n",
    "        avg_sc = np.mean([r.overall_score for r in diff_results])\n",
    "        avg_h = np.mean([r.hallucination_score for r in diff_results])\n",
    "        print(f\"{diff:<12s} {n:>3d} {pass_pct:>5.0f}% {avg_sc:>5.1f} {avg_h:>6.0%}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# \u2500\u2500 Failed Prompts Detail \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "failed = [r for r in results if not r.passed]\n",
    "if failed:\n",
    "    print(f\"\\nFAILED PROMPTS ({len(failed)}):\")\n",
    "    print(\"\u2500\" * 60)\n",
    "    for r in failed:\n",
    "        print(f\"\\n  {r.prompt_id} [{r.category} / {r.difficulty}]\")\n",
    "        print(f\"  Prompt: {r.prompt[:100]}\")\n",
    "        print(f\"  Score: {r.overall_score:.0f}/100  |  Tool: {r.tool_accuracy:.0%}  |  Hall: {r.hallucination_score:.0%}\")\n",
    "        if r.hallucination_flags:\n",
    "            print(f\"  Hallucination flags: {r.hallucination_flags[:3]}\")\n",
    "        if r.error:\n",
    "            print(f\"  Error: {r.error[:150]}\")\n",
    "else:\n",
    "    print(\"\\nAll prompts passed.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# \u2500\u2500 Display results table \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "display_df = results_df[[\n",
    "    \"prompt_id\", \"category\", \"difficulty\", \"passed\", \"overall_score\",\n",
    "    \"tool_accuracy\", \"keyword_score\", \"hallucination_score\", \"groundedness_score\",\n",
    "    \"latency_seconds\"\n",
    "]].copy()\n",
    "display_df.columns = [\"ID\", \"Category\", \"Difficulty\", \"Passed\", \"Score\",\n",
    "                       \"Tool Acc\", \"Keywords\", \"Hallucination\", \"Grounded\", \"Latency (s)\"]\n",
    "\n",
    "for col in [\"Score\", \"Tool Acc\", \"Keywords\", \"Hallucination\", \"Grounded\", \"Latency (s)\"]:\n",
    "    display_df[col] = display_df[col].round(2)\n",
    "\n",
    "spark_display = spark.createDataFrame(display_df)\n",
    "display(spark_display)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results to Delta\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "EVAL_TABLE = f\"{CATALOG}.gold.agent_evaluation_results\"\n",
    "\n",
    "spark_results = spark.createDataFrame(results_df)\n",
    "\n",
    "# Append results (accumulate over time for trend analysis)\n",
    "spark_results.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(EVAL_TABLE)\n",
    "\n",
    "print(f\"Saved {len(results_df)} evaluation results to {EVAL_TABLE}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical Trend Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load all historical results\n",
    "try:\n",
    "    hist_df = spark.table(EVAL_TABLE).toPandas()\n",
    "    hist_df[\"timestamp\"] = pd.to_datetime(hist_df[\"timestamp\"])\n",
    "    \n",
    "    # Group by run\n",
    "    runs = hist_df.groupby(\"run_id\").agg(\n",
    "        timestamp=(\"timestamp\", \"first\"),\n",
    "        n_prompts=(\"prompt_id\", \"count\"),\n",
    "        pass_rate=(\"passed\", \"mean\"),\n",
    "        avg_score=(\"overall_score\", \"mean\"),\n",
    "        avg_tool_accuracy=(\"tool_accuracy\", \"mean\"),\n",
    "        avg_hallucination=(\"hallucination_score\", \"mean\"),\n",
    "        avg_latency=(\"latency_seconds\", \"mean\"),\n",
    "    ).sort_values(\"timestamp\")\n",
    "    \n",
    "    print(\"EVALUATION HISTORY:\")\n",
    "    print(f\"{'Run ID':<14s} {'Date':<20s} {'N':>3s} {'Pass%':>6s} {'Score':>6s} {'ToolAcc':>8s} {'Halluc':>7s} {'Latency':>8s}\")\n",
    "    print(\"\u2500\" * 80)\n",
    "    for _, row in runs.iterrows():\n",
    "        print(f\"{row.name:<14s} {row['timestamp'].strftime('%Y-%m-%d %H:%M'):<20s} \"\n",
    "              f\"{row['n_prompts']:>3.0f} {row['pass_rate']*100:>5.0f}% {row['avg_score']:>5.1f} \"\n",
    "              f\"{row['avg_tool_accuracy']:>7.0%} {row['avg_hallucination']:>6.0%} {row['avg_latency']:>7.1f}s\")\n",
    "    \n",
    "    # Trend\n",
    "    if len(runs) >= 2:\n",
    "        first_score = runs.iloc[0][\"avg_score\"]\n",
    "        last_score = runs.iloc[-1][\"avg_score\"]\n",
    "        delta = last_score - first_score\n",
    "        print(f\"\\nTrend: {delta:+.1f} points since first evaluation\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"No historical data yet (first run): {e}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Prompt Reference\n",
    "\n",
    "| ID | Category | Difficulty | Prompt | Expected Tools |\n",
    "|----|----------|------------|--------|----------------|\n",
    "| FC-001 | Forecasting | Easy | Demand forecast next 3 months | `get_demand_forecast` |\n",
    "| FC-002 | Forecasting | Medium | Compare Prophet/ARIMA/RF | `compare_forecast_models` |\n",
    "| FC-003 | Forecasting | Medium | Forecast confidence assessment | `assess_forecast_confidence` |\n",
    "| FC-004 | Forecasting | Easy | 12-month forecast with CIs | `get_demand_forecast` |\n",
    "| AN-001 | Analysis | Easy | Demand anomalies last 6 months | `detect_anomalies` |\n",
    "| AN-002 | Analysis | Medium | Trend analysis 12 months | `detect_trends` |\n",
    "| AN-003 | Analysis | Medium | Top demand drivers | `explain_demand_drivers` |\n",
    "| AN-004 | Analysis | Easy | Anomalies with 10% threshold | `detect_anomalies` |\n",
    "| SC-001 | Scenarios | Easy | CRITICAL geo risk in Europe | `scenario_geopolitical_risk` |\n",
    "| SC-002 | Scenarios | Easy | 50% tariff increase on steel | `scenario_tariff_increase` |\n",
    "| SC-003 | Scenarios | Easy | Hurricane in Gulf Coast | `scenario_weather_disruption` |\n",
    "| SC-004 | Scenarios | Hard | Combined multi-factor scenario | `build_whatif_scenario` |\n",
    "| SC-005 | Scenarios | Hard | Dual scenario: winter + geo risk | `scenario_weather_disruption` + `scenario_geopolitical_risk` |\n",
    "| IN-001 | Intelligence | Easy | Suppliers in Wisconsin | `query_suppliers` |\n",
    "| IN-002 | Intelligence | Easy | Contracts PSC 23, FY2024 | `search_contracts` |\n",
    "| IN-003 | Intelligence | Medium | DoD metrics vs objectives | `compare_dod_metrics` |\n",
    "| IN-004 | Intelligence | Medium | Industrial metals prices | `get_commodity_prices` |\n",
    "| IN-005 | Intelligence | Medium | GSCPI and trade indicators | `get_macro_context` |\n",
    "| IN-006 | Intelligence | Medium | Small business armor suppliers | `query_suppliers` |\n",
    "| EX-001 | Executive | Easy | Health dashboard | `get_supply_chain_health` |\n",
    "| EX-002 | Executive | Easy | Executive briefing | `generate_executive_briefing` |\n",
    "| EX-003 | Executive | Medium | Leadership briefing | `generate_executive_briefing` |\n",
    "| MX-001 | Multi-Tool | Hard | Full picture: forecast + confidence + anomalies | 3 tools |\n",
    "| MX-002 | Multi-Tool | Hard | Health + DoD metrics combined | 2 tools |\n",
    "| MX-003 | Multi-Tool | Hard | Commodity trends + demand correlation | 2 tools |\n",
    "| HT-001 | Hallucination Trap | Hard | Exact daily demand (data is monthly) | None expected |\n",
    "| HT-002 | Hallucination Trap | Hard | CEO name of supplier (not in data) | `query_suppliers` |\n",
    "| HT-003 | Hallucination Trap | Hard | Foreign operations (not in data) | None expected |\n"
   ]
  }
 ]
}