{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WTO Goods Trade Barometer Ingestion\n",
    "\n",
    "**Executive summary:** Ingests WTO Goods Trade Barometer (quarterly) when available via API. WTO Timeseries API may require a free API key from https://apiportal.wto.org/. Fails gracefully with clear message if data is not available programmatically (no scraping).\n",
    "\n",
    "**Data Source**: WTO Stats/Timeseries API. Tries public endpoints first; if API key is set (e.g. WTO_API_KEY), uses it. Otherwise raises with instructions.\n",
    "\n",
    "**Target Tables** (Unity Catalog):\n",
    "- `supply_chain.raw.wto_trade_barometer` - Raw barometer observations\n",
    "- `supply_chain.bronze.wto_trade_barometer` - Cleaned with typed columns\n",
    "\n",
    "**Idempotency:** Delta merge on (source, indicator_code, country_code, as_of_date). country_code = \"\" (global).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, DoubleType, TimestampType, DateType,\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "CATALOG = \"supply_chain\"\n",
    "RAW_TABLE = f\"{CATALOG}.raw.wto_trade_barometer\"\n",
    "BRONZE_TABLE = f\"{CATALOG}.bronze.wto_trade_barometer\"\n",
    "\n",
    "# WTO API key (optional): set env WTO_API_KEY or spark.conf for programmatic access\n",
    "try:\n",
    "    _key = spark.conf.get(\"wto.api.key\") if spark else \"\"\n",
    "except Exception:\n",
    "    _key = \"\"\n",
    "WTO_API_KEY = os.environ.get(\"WTO_API_KEY\", \"\") or _key or \"\"\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared HTTP helper\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import sys\n",
    "import os\n",
    "for _p in [os.path.dirname(os.path.abspath(__file__)) if \"__file__\" in dir() else \"\", os.getcwd(), \"/Workspace/Repos\", \".\"]:\n",
    "    if _p and _p not in sys.path:\n",
    "        sys.path.insert(0, _p)\n",
    "try:\n",
    "    from ingestion_utils import safe_get, parse_json, normalize_indicator_row\n",
    "except ImportError:\n",
    "    import requests\n",
    "    import time\n",
    "    def safe_get(url, *, timeout=60, retries=3, backoff=2.0, headers=None):\n",
    "        last = None\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                r = requests.get(url, timeout=timeout, headers=headers or {})\n",
    "                r.raise_for_status()\n",
    "                return r\n",
    "            except Exception as e:\n",
    "                last = e\n",
    "                if attempt < retries - 1:\n",
    "                    time.sleep(backoff ** attempt)\n",
    "        raise last\n",
    "    def parse_json(text):\n",
    "        return json.loads(text)\n",
    "    def normalize_indicator_row(*, source, ingested_at, as_of_date, country_code, indicator_code, indicator_name, value, unit, frequency, raw_payload=None):\n",
    "        return {\"source\": source, \"ingested_at\": ingested_at, \"as_of_date\": as_of_date, \"country_code\": country_code or \"\", \"indicator_code\": indicator_code, \"indicator_name\": indicator_name, \"value\": float(value) if value is not None else None, \"unit\": unit, \"frequency\": frequency, \"raw_payload\": raw_payload}\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch WTO Trade Barometer\n",
    "\n",
    "Tries public data URL or WTO API (with key). Fails with clear message if not feasible without scraping.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def fetch_wto_trade_barometer() -> list:\n",
    "    \"\"\"\n",
    "    Fetch WTO Goods Trade Barometer (quarterly). Uses public export or API with key.\n",
    "    Returns list of normalized row dicts; country_code = \"\".\n",
    "    \"\"\"\n",
    "    ingested_at = datetime.utcnow().isoformat() + \"Z\"\n",
    "    rows = []\n",
    "\n",
    "    # Try 1: Public bulk/download endpoint (if WTO exposes barometer without key)\n",
    "    public_urls = [\n",
    "        \"https://stats.wto.org/api/v1/barometer\",\n",
    "        \"https://api.wto.org/timeseries/v1/data\",\n",
    "    ]\n",
    "    headers = {}\n",
    "    if WTO_API_KEY:\n",
    "        headers[\"Ocp-Apim-Subscription-Key\"] = WTO_API_KEY\n",
    "        headers[\"Authorization\"] = f\"Bearer {WTO_API_KEY}\"\n",
    "\n",
    "    for url in public_urls:\n",
    "        try:\n",
    "            r = safe_get(url, timeout=60, headers=headers if headers else None)\n",
    "            text = r.text\n",
    "            if not text or len(text) < 10:\n",
    "                continue\n",
    "            data = parse_json(text)\n",
    "            # Normalize: expect list of {period, value} or {date, barometer_value} etc.\n",
    "            if isinstance(data, list):\n",
    "                for rec in data:\n",
    "                    if not isinstance(rec, dict):\n",
    "                        continue\n",
    "                    period = rec.get(\"Period\") or rec.get(\"period\") or rec.get(\"Date\") or rec.get(\"date\") or rec.get(\"Time Period\")\n",
    "                    val = rec.get(\"Value\") or rec.get(\"value\") or rec.get(\"Barometer\") or rec.get(\"barometer\")\n",
    "                    if period is None or val is None:\n",
    "                        continue\n",
    "                    try:\n",
    "                        value_float = float(val)\n",
    "                    except (TypeError, ValueError):\n",
    "                        continue\n",
    "                    # Quarter to first day: e.g. 2024Q1 -> 2024-01-01\n",
    "                    ps = str(period).upper()\n",
    "                    if \"Q\" in ps:\n",
    "                        parts = ps.split(\"Q\")\n",
    "                        y = parts[0][:4]\n",
    "                        q = parts[1][:1] if len(parts) > 1 else \"1\"\n",
    "                        m = {\"1\": \"01\", \"2\": \"04\", \"3\": \"07\", \"4\": \"10\"}.get(q, \"01\")\n",
    "                        as_of_date = f\"{y}-{m}-01\"\n",
    "                    else:\n",
    "                        as_of_date = str(period)[:10] if len(str(period)) >= 10 else f\"{period}-01-01\"\n",
    "                    row = normalize_indicator_row(\n",
    "                        source=\"wto_trade_barometer\",\n",
    "                        ingested_at=ingested_at,\n",
    "                        as_of_date=as_of_date,\n",
    "                        country_code=\"\",\n",
    "                        indicator_code=\"WTO_GOODS_BAROMETER\",\n",
    "                        indicator_name=\"WTO Goods Trade Barometer\",\n",
    "                        value=value_float,\n",
    "                        unit=\"index\",\n",
    "                        frequency=\"quarterly\",\n",
    "                        raw_payload=json.dumps(rec),\n",
    "                    )\n",
    "                    rows.append(row)\n",
    "                if rows:\n",
    "                    return rows\n",
    "            if isinstance(data, dict):\n",
    "                # Single value or nested\n",
    "                for k, v in data.items():\n",
    "                    if k in (\"metadata\", \"Meta\", \"info\"):\n",
    "                        continue\n",
    "                    if isinstance(v, (int, float)):\n",
    "                        row = normalize_indicator_row(\n",
    "                            source=\"wto_trade_barometer\",\n",
    "                            ingested_at=ingested_at,\n",
    "                            as_of_date=datetime.utcnow().strftime(\"%Y-%m-%d\"),\n",
    "                            country_code=\"\",\n",
    "                            indicator_code=\"WTO_GOODS_BAROMETER\",\n",
    "                            indicator_name=\"WTO Goods Trade Barometer\",\n",
    "                            value=float(v),\n",
    "                            unit=\"index\",\n",
    "                            frequency=\"quarterly\",\n",
    "                            raw_payload=json.dumps(data),\n",
    "                        )\n",
    "                        rows.append(row)\n",
    "                        return rows\n",
    "        except Exception as e:\n",
    "            print(f\"WTO URL {url[:50]}... failed: {e}\")\n",
    "            continue\n",
    "\n",
    "    # No data without scraping\n",
    "    raise RuntimeError(\n",
    "        \"WTO Goods Trade Barometer data is not available programmatically with current configuration. \"\n",
    "        \"Options: (1) Sign up for a free API key at https://apiportal.wto.org/ and set WTO_API_KEY (env or spark.conf 'wto.api.key'), \"\n",
    "        \"(2) Download data manually from https://www.wto.org/english/res_e/statis_e/wtoi_e.htm and load into this table, \"\n",
    "        \"(3) Skip this notebook if WTO barometer is not required.\"\n",
    "    ) from None\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "try:\n",
    "    all_rows = fetch_wto_trade_barometer()\n",
    "    print(f\"Fetched {len(all_rows)} WTO Trade Barometer records\")\n",
    "except RuntimeError as e:\n",
    "    print(str(e))\n",
    "    raise\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Spark DataFrame and schema\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "INDICATOR_RAW_SCHEMA = StructType([\n",
    "    StructField(\"source\", StringType(), False),\n",
    "    StructField(\"ingested_at\", StringType(), False),\n",
    "    StructField(\"as_of_date\", StringType(), False),\n",
    "    StructField(\"country_code\", StringType(), False),\n",
    "    StructField(\"indicator_code\", StringType(), False),\n",
    "    StructField(\"indicator_name\", StringType(), False),\n",
    "    StructField(\"value\", DoubleType(), True),\n",
    "    StructField(\"unit\", StringType(), True),\n",
    "    StructField(\"frequency\", StringType(), False),\n",
    "    StructField(\"raw_payload\", StringType(), True),\n",
    "])\n",
    "\n",
    "df_raw = spark.createDataFrame(all_rows, INDICATOR_RAW_SCHEMA)\n",
    "df_raw = df_raw.withColumn(\"ingested_at\", F.col(\"ingested_at\").cast(TimestampType()))\n",
    "df_raw = df_raw.withColumn(\"as_of_date\", F.to_date(F.col(\"as_of_date\")))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unity Catalog and idempotent merge (raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.raw\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.bronze\")\n",
    "\n",
    "raw_create_sql = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {RAW_TABLE} (\n",
    "  source STRING NOT NULL,\n",
    "  ingested_at TIMESTAMP NOT NULL,\n",
    "  as_of_date DATE NOT NULL,\n",
    "  country_code STRING NOT NULL,\n",
    "  indicator_code STRING NOT NULL,\n",
    "  indicator_name STRING NOT NULL,\n",
    "  value DOUBLE,\n",
    "  unit STRING,\n",
    "  frequency STRING NOT NULL,\n",
    "  raw_payload STRING\n",
    ") USING DELTA\n",
    "\"\"\"\n",
    "spark.sql(raw_create_sql)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "dt_raw = DeltaTable.forName(spark, RAW_TABLE)\n",
    "dt_raw.alias(\"t\").merge(\n",
    "    df_raw.alias(\"s\"),\n",
    "    \"t.source = s.source AND t.indicator_code = s.indicator_code AND t.country_code = s.country_code AND t.as_of_date = s.as_of_date\"\n",
    ").whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "\n",
    "raw_count = spark.table(RAW_TABLE).count()\n",
    "print(f\"Raw table row count after merge: {raw_count}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bronze\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "bronze_df = spark.table(RAW_TABLE).select(\n",
    "    F.col(\"source\"), F.col(\"ingested_at\"), F.col(\"as_of_date\"), F.col(\"country_code\"),\n",
    "    F.col(\"indicator_code\"), F.col(\"indicator_name\"), F.col(\"value\"), F.col(\"unit\"),\n",
    "    F.col(\"frequency\"), F.col(\"raw_payload\"),\n",
    ")\n",
    "bronze_create_sql = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {BRONZE_TABLE} (\n",
    "  source STRING NOT NULL,\n",
    "  ingested_at TIMESTAMP NOT NULL,\n",
    "  as_of_date DATE NOT NULL,\n",
    "  country_code STRING NOT NULL,\n",
    "  indicator_code STRING NOT NULL,\n",
    "  indicator_name STRING NOT NULL,\n",
    "  value DOUBLE,\n",
    "  unit STRING,\n",
    "  frequency STRING NOT NULL,\n",
    "  raw_payload STRING\n",
    ") USING DELTA\n",
    "\"\"\"\n",
    "spark.sql(bronze_create_sql)\n",
    "dt_bronze = DeltaTable.forName(spark, BRONZE_TABLE)\n",
    "dt_bronze.alias(\"t\").merge(\n",
    "    bronze_df.alias(\"s\"),\n",
    "    \"t.source = s.source AND t.indicator_code = s.indicator_code AND t.country_code = s.country_code AND t.as_of_date = s.as_of_date\"\n",
    ").whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "\n",
    "bronze_count = spark.table(BRONZE_TABLE).count()\n",
    "print(f\"Bronze table row count after merge: {bronze_count}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log row counts and sample\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"=== WTO Trade Barometer Ingestion ===\")\n",
    "print(f\"Raw {RAW_TABLE}: {raw_count} rows\")\n",
    "print(f\"Bronze {BRONZE_TABLE}: {bronze_count} rows\")\n",
    "display(spark.table(RAW_TABLE).orderBy(F.desc(\"as_of_date\")).limit(10))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tables: `supply_chain.raw.wto_trade_barometer`, `supply_chain.bronze.wto_trade_barometer`. Idempotent merge on (source, indicator_code, country_code, as_of_date).\n"
   ]
  }
 ]
}