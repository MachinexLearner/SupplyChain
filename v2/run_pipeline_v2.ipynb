{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supply Chain Pipeline Orchestrator (v2)\n",
    "\n",
    "**Run the entire v2 pipeline with a single click.** This notebook executes all\n",
    "ingestion, transformation, and forecasting notebooks in the correct order using\n",
    "`dbutils.notebook.run()`.\n",
    "\n",
    "### Pipeline Stages\n",
    "| Stage | Notebooks | Execution |\n",
    "|-------|-----------|-----------|\n",
    "| **Setup** | `00_setup_catalog_v2` | Sequential (must run first) |\n",
    "| **Ingestion** | 01\u201304, 06\u201310, 12\u201313 | Parallel (independent tables) |\n",
    "| **Transformation** | `01_unified_demand_signals_v2` \u2192 `02_dod_metrics_inputs_v2` | Sequential |\n",
    "| **Forecasting** | Prophet, ARIMA, Random Forest | Parallel |\n",
    "\n",
    "### Widgets\n",
    "- **run_stages**: Comma-separated stages to run (`setup,ingestion,transformation,forecasting`)\n",
    "- **parallel_ingestion**: Run ingestion notebooks in parallel (`true`/`false`)\n",
    "- **timeout_seconds**: Timeout per notebook in seconds\n",
    "- **continue_on_error**: Continue pipeline if a notebook fails (`true`/`false`)\n",
    "- **sam_api_key**: SAM.gov API key (passed to `04_sam_entity_ingestion_v2`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dbutils.widgets.text(\"run_stages\", \"setup,ingestion,transformation,forecasting\", \"Stages to run (comma-separated)\")\n",
    "dbutils.widgets.dropdown(\"parallel_ingestion\", \"true\", [\"true\", \"false\"], \"Parallel ingestion?\")\n",
    "dbutils.widgets.text(\"timeout_seconds\", \"3600\", \"Timeout per notebook (seconds)\")\n",
    "dbutils.widgets.dropdown(\"continue_on_error\", \"false\", [\"true\", \"false\"], \"Continue on error?\")\n",
    "dbutils.widgets.text(\"sam_api_key\", \"\", \"SAM.gov API key (optional override)\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import time\n",
    "import traceback\n",
    "from datetime import datetime, timezone\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Parse widgets\n",
    "RUN_STAGES = [s.strip().lower() for s in dbutils.widgets.get(\"run_stages\").split(\",\")]\n",
    "PARALLEL_INGESTION = dbutils.widgets.get(\"parallel_ingestion\").lower() == \"true\"\n",
    "TIMEOUT = int(dbutils.widgets.get(\"timeout_seconds\"))\n",
    "CONTINUE_ON_ERROR = dbutils.widgets.get(\"continue_on_error\").lower() == \"true\"\n",
    "SAM_API_KEY = dbutils.widgets.get(\"sam_api_key\").strip()\n",
    "\n",
    "print(f\"Pipeline started at {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')}\")\n",
    "print(f\"  Stages       : {RUN_STAGES}\")\n",
    "print(f\"  Parallel ingest: {PARALLEL_INGESTION}\")\n",
    "print(f\"  Timeout/nb   : {TIMEOUT}s\")\n",
    "print(f\"  Continue err  : {CONTINUE_ON_ERROR}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# \u2500\u2500 Notebook paths (relative to this notebook's location) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "SETUP_NOTEBOOKS = [\n",
    "    \"ingestion/00_setup_catalog_v2\",\n",
    "]\n",
    "\n",
    "INGESTION_NOTEBOOKS = [\n",
    "    \"ingestion/01_usaspending_ingestion_v2\",\n",
    "    \"ingestion/02_fpds_ingestion_v2\",\n",
    "    \"ingestion/03_subaward_ingestion_v2\",\n",
    "    \"ingestion/04_sam_entity_ingestion_v2\",\n",
    "    \"ingestion/06_tariff_trade_ingestion_v2\",\n",
    "    \"ingestion/07_commodity_ingestion_v2\",\n",
    "    \"ingestion/08_weather_ingestion_v2\",\n",
    "    \"ingestion/09_worldbank_wdi_ingestion_v2\",\n",
    "    \"ingestion/10_worldbank_wgi_ingestion_v2\",\n",
    "    \"ingestion/12_nyfed_gscpi_ingestion_v2\",\n",
    "    \"ingestion/13_wto_trade_barometer_ingestion_v2\",\n",
    "]\n",
    "\n",
    "TRANSFORMATION_NOTEBOOKS = [\n",
    "    \"transformation/01_unified_demand_signals_v2\",\n",
    "    \"transformation/02_dod_metrics_inputs_v2\",\n",
    "]\n",
    "\n",
    "FORECASTING_NOTEBOOKS = [\n",
    "    \"forecasting/01_prophet_forecasting_v2\",\n",
    "    \"forecasting/02_arima_forecasting_v2\",\n",
    "    \"forecasting/03_random_forest_forecasting_v2\",\n",
    "]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution Helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class NotebookResult:\n",
    "    \"\"\"Tracks the result of a single notebook run.\"\"\"\n",
    "    def __init__(self, path: str, status: str = \"pending\", duration: float = 0.0, error: str = \"\"):\n",
    "        self.path = path\n",
    "        self.status = status       # pending | running | success | failed | skipped\n",
    "        self.duration = duration   # seconds\n",
    "        self.error = error\n",
    "\n",
    "    def __repr__(self):\n",
    "        icon = {\"success\": \"\u2705\", \"failed\": \"\u274c\", \"skipped\": \"\u23ed\ufe0f\", \"running\": \"\ud83d\udd04\"}.get(self.status, \"\u23f3\")\n",
    "        dur = f\" ({self.duration:.1f}s)\" if self.duration else \"\"\n",
    "        err = f\" \u2014 {self.error[:120]}\" if self.error else \"\"\n",
    "        return f\"{icon} {self.path}: {self.status}{dur}{err}\"\n",
    "\n",
    "\n",
    "def run_notebook(path: str, extra_params: dict = None) -> NotebookResult:\n",
    "    \"\"\"Run a single notebook and return its result.\"\"\"\n",
    "    params = extra_params or {}\n",
    "    result = NotebookResult(path, status=\"running\")\n",
    "    start = time.time()\n",
    "    try:\n",
    "        dbutils.notebook.run(f\"./{path}\", TIMEOUT, params)\n",
    "        result.status = \"success\"\n",
    "    except Exception as e:\n",
    "        result.status = \"failed\"\n",
    "        result.error = str(e)\n",
    "        if not CONTINUE_ON_ERROR:\n",
    "            raise\n",
    "    finally:\n",
    "        result.duration = time.time() - start\n",
    "    return result\n",
    "\n",
    "\n",
    "def run_sequential(notebooks: list, stage_name: str, extra_params: dict = None) -> list:\n",
    "    \"\"\"Run notebooks one after another. Returns list of NotebookResult.\"\"\"\n",
    "    results = []\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"  Stage: {stage_name} (sequential, {len(notebooks)} notebooks)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for nb in notebooks:\n",
    "        print(f\"  \u25b6 Running {nb} ...\")\n",
    "        r = run_notebook(nb, extra_params)\n",
    "        results.append(r)\n",
    "        print(f\"    {r}\")\n",
    "        if r.status == \"failed\" and not CONTINUE_ON_ERROR:\n",
    "            print(f\"  \u26d4 Stopping pipeline \u2014 {nb} failed.\")\n",
    "            break\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_parallel(notebooks: list, stage_name: str, extra_params: dict = None) -> list:\n",
    "    \"\"\"Run notebooks concurrently using ThreadPoolExecutor. Returns list of NotebookResult.\"\"\"\n",
    "    results = []\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"  Stage: {stage_name} (parallel, {len(notebooks)} notebooks)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    with ThreadPoolExecutor(max_workers=len(notebooks)) as executor:\n",
    "        futures = {executor.submit(run_notebook, nb, extra_params): nb for nb in notebooks}\n",
    "        for future in as_completed(futures):\n",
    "            nb = futures[future]\n",
    "            try:\n",
    "                r = future.result()\n",
    "            except Exception as e:\n",
    "                r = NotebookResult(nb, status=\"failed\", error=str(e))\n",
    "            results.append(r)\n",
    "            print(f\"    {r}\")\n",
    "    return results\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pipeline_start = time.time()\n",
    "all_results = []\n",
    "\n",
    "# \u2500\u2500 Stage 1: Setup \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "if \"setup\" in RUN_STAGES:\n",
    "    all_results += run_sequential(SETUP_NOTEBOOKS, \"Setup\")\n",
    "    if any(r.status == \"failed\" for r in all_results) and not CONTINUE_ON_ERROR:\n",
    "        dbutils.notebook.exit(\"FAILED at Setup stage\")\n",
    "else:\n",
    "    print(\"\\n\u23ed\ufe0f  Skipping Setup stage\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# \u2500\u2500 Stage 2: Ingestion \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "if \"ingestion\" in RUN_STAGES:\n",
    "    # Build extra params for SAM notebook\n",
    "    ingestion_params = {}\n",
    "    if SAM_API_KEY:\n",
    "        ingestion_params[\"sam_api_key\"] = SAM_API_KEY\n",
    "\n",
    "    if PARALLEL_INGESTION:\n",
    "        # All ingestion notebooks write to independent tables so they can run in parallel\n",
    "        all_results += run_parallel(INGESTION_NOTEBOOKS, \"Ingestion\", ingestion_params)\n",
    "    else:\n",
    "        all_results += run_sequential(INGESTION_NOTEBOOKS, \"Ingestion\", ingestion_params)\n",
    "\n",
    "    failed_ingestion = [r for r in all_results if r.status == \"failed\"]\n",
    "    if failed_ingestion and not CONTINUE_ON_ERROR:\n",
    "        dbutils.notebook.exit(f\"FAILED at Ingestion stage: {[r.path for r in failed_ingestion]}\")\n",
    "else:\n",
    "    print(\"\\n\u23ed\ufe0f  Skipping Ingestion stage\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# \u2500\u2500 Stage 3: Transformation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "if \"transformation\" in RUN_STAGES:\n",
    "    all_results += run_sequential(TRANSFORMATION_NOTEBOOKS, \"Transformation\")\n",
    "    if any(r.status == \"failed\" for r in all_results if r.path.startswith(\"transformation/\")) and not CONTINUE_ON_ERROR:\n",
    "        dbutils.notebook.exit(\"FAILED at Transformation stage\")\n",
    "else:\n",
    "    print(\"\\n\u23ed\ufe0f  Skipping Transformation stage\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# \u2500\u2500 Stage 4: Forecasting \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "if \"forecasting\" in RUN_STAGES:\n",
    "    # Prophet, ARIMA, and RF are independent \u2014 run in parallel\n",
    "    all_results += run_parallel(FORECASTING_NOTEBOOKS, \"Forecasting\")\n",
    "else:\n",
    "    print(\"\\n\u23ed\ufe0f  Skipping Forecasting stage\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pipeline_duration = time.time() - pipeline_start\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"  PIPELINE COMPLETE \u2014 {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')}\")\n",
    "print(f\"  Total duration: {pipeline_duration:.1f}s ({pipeline_duration/60:.1f} min)\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "succeeded = [r for r in all_results if r.status == \"success\"]\n",
    "failed    = [r for r in all_results if r.status == \"failed\"]\n",
    "skipped   = [r for r in all_results if r.status == \"skipped\"]\n",
    "\n",
    "print(f\"  \u2705 Succeeded: {len(succeeded)}\")\n",
    "print(f\"  \u274c Failed:    {len(failed)}\")\n",
    "print(f\"  \u23ed\ufe0f  Skipped:   {len(skipped)}\")\n",
    "print()\n",
    "\n",
    "for r in all_results:\n",
    "    print(f\"  {r}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Build summary DataFrame for display\n",
    "import pandas as pd\n",
    "\n",
    "summary_data = [{\n",
    "    \"Notebook\": r.path,\n",
    "    \"Status\": r.status,\n",
    "    \"Duration (s)\": round(r.duration, 1),\n",
    "    \"Error\": r.error[:200] if r.error else \"\"\n",
    "} for r in all_results]\n",
    "\n",
    "summary_df = spark.createDataFrame(pd.DataFrame(summary_data))\n",
    "display(summary_df)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Exit with status\n",
    "if failed:\n",
    "    msg = f\"FAILED: {len(failed)} notebook(s) failed \u2014 {[r.path for r in failed]}\"\n",
    "    print(f\"\\n\u26d4 {msg}\")\n",
    "    dbutils.notebook.exit(msg)\n",
    "else:\n",
    "    msg = f\"SUCCESS: All {len(succeeded)} notebooks completed in {pipeline_duration:.1f}s\"\n",
    "    print(f\"\\n\u2705 {msg}\")\n",
    "    dbutils.notebook.exit(msg)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Deploy as Databricks Workflow Job\n",
    "\n",
    "To schedule this pipeline as an automated Databricks Workflow, run the cell\n",
    "below. It reads the job definition from `jobs/supply_chain_full_pipeline_v2.json`\n",
    "and creates (or resets) the Workflow via the Jobs API.\n",
    "\n",
    "**Prerequisites:**\n",
    "- Update `notebook_path` values in the JSON to match your workspace location\n",
    "- The cluster running this cell needs permission to call the Jobs API\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# import requests, json, os\n",
    "#\n",
    "# # \u2500\u2500 Uncomment and run this cell to deploy the Workflow job \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "#\n",
    "# # Read the job definition\n",
    "# job_json_path = os.path.join(\n",
    "#     os.path.dirname(dbutils.notebook.entry_point.getDbutils()\n",
    "#         .notebook().getContext().notebookPath().get()),\n",
    "#     \"jobs\", \"supply_chain_full_pipeline_v2.json\"\n",
    "# )\n",
    "#\n",
    "# # Alternative: read from local filesystem if synced\n",
    "# # with open(\"/Workspace/SupplyChain/notebooks/v2/jobs/supply_chain_full_pipeline_v2.json\") as f:\n",
    "# #     job_def = json.load(f)\n",
    "#\n",
    "# host = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()\n",
    "# token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "# headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n",
    "#\n",
    "# # Check if job already exists\n",
    "# resp = requests.get(f\"{host}/api/2.1/jobs/list\", headers=headers, params={\"name\": \"supply_chain_full_pipeline_v2\"})\n",
    "# existing = resp.json().get(\"jobs\", [])\n",
    "#\n",
    "# if existing:\n",
    "#     job_id = existing[0][\"job_id\"]\n",
    "#     print(f\"Job already exists (id={job_id}). Resetting definition...\")\n",
    "#     requests.post(f\"{host}/api/2.1/jobs/reset\", headers=headers, json={\"job_id\": job_id, **job_def})\n",
    "#     print(f\"\u2705 Job {job_id} updated.\")\n",
    "# else:\n",
    "#     resp = requests.post(f\"{host}/api/2.1/jobs/create\", headers=headers, json=job_def)\n",
    "#     job_id = resp.json().get(\"job_id\")\n",
    "#     print(f\"\u2705 Job created with id={job_id}\")\n",
    "#\n",
    "# print(f\"View job: {host}/#job/{job_id}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}