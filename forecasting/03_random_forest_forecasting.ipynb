{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Demand Forecasting with Feature Importance\n",
    "\n",
    "**Executive summary:** Implements Random Forest regression for demand forecasting with comprehensive feature engineering and explainability. Unlike Prophet/ARIMA, RF captures complex non-linear relationships between demand and risk signals. Management: use for feature importance insights and scenario modeling.\n",
    "\n",
    "**Depends on:** `gold.oshkosh_monthly_demand_signals` (from 01_unified_demand_signals)\n",
    "\n",
    "**Outputs:**\n",
    "- `gold.random_forest_forecasts` - Forecast predictions with confidence intervals\n",
    "- `gold.random_forest_feature_importance` - Feature importance rankings\n",
    "- MLflow logged model with metrics\n",
    "\n",
    "**Key Features:**\n",
    "- Engineered lag features (1, 3, 6, 12 months)\n",
    "- Rolling statistics (mean, std, trend)\n",
    "- Seasonal indicators\n",
    "- Risk signal interactions\n",
    "- SHAP values for explainability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%pip install scikit-learn shap mlflow pandas numpy matplotlib seaborn\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import shap\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Configuration - Unity Catalog\n",
    "CATALOG = \"supply_chain\"\n",
    "DEMAND_SIGNALS_TABLE = f\"{CATALOG}.gold.oshkosh_monthly_demand_signals\"\n",
    "FORECAST_OUTPUT_TABLE = f\"{CATALOG}.gold.random_forest_forecasts\"\n",
    "FEATURE_IMPORTANCE_TABLE = f\"{CATALOG}.gold.random_forest_feature_importance\"\n",
    "\n",
    "# Model parameters\n",
    "FORECAST_HORIZON = 12  # months ahead\n",
    "TEST_SIZE = 12  # months for testing\n",
    "N_ESTIMATORS = 200\n",
    "MAX_DEPTH = 15\n",
    "MIN_SAMPLES_SPLIT = 5\n",
    "RANDOM_STATE = 42\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Data\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load demand signals\n",
    "demand_df = spark.table(DEMAND_SIGNALS_TABLE).toPandas()\n",
    "demand_df['month'] = pd.to_datetime(demand_df['month'])\n",
    "demand_df = demand_df.sort_values('month').reset_index(drop=True)\n",
    "\n",
    "print(f\"\u2713 Loaded {len(demand_df)} months of data\")\n",
    "print(f\"  Date range: {demand_df['month'].min()} to {demand_df['month'].max()}\")\n",
    "print(f\"  Columns: {len(demand_df.columns)}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def engineer_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create comprehensive feature set for Random Forest.\n",
    "    \n",
    "    Features include:\n",
    "    - Lag features (1, 3, 6, 12 months)\n",
    "    - Rolling statistics (mean, std, min, max)\n",
    "    - Trend indicators\n",
    "    - Seasonal features\n",
    "    - Risk signal interactions\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Target variable\n",
    "    target_col = 'total_obligations_usd'\n",
    "    \n",
    "    # === LAG FEATURES ===\n",
    "    for lag in [1, 3, 6, 12]:\n",
    "        df[f'demand_lag_{lag}m'] = df[target_col].shift(lag)\n",
    "    \n",
    "    # === ROLLING STATISTICS ===\n",
    "    for window in [3, 6, 12]:\n",
    "        df[f'demand_rolling_mean_{window}m'] = df[target_col].shift(1).rolling(window).mean()\n",
    "        df[f'demand_rolling_std_{window}m'] = df[target_col].shift(1).rolling(window).std()\n",
    "        df[f'demand_rolling_min_{window}m'] = df[target_col].shift(1).rolling(window).min()\n",
    "        df[f'demand_rolling_max_{window}m'] = df[target_col].shift(1).rolling(window).max()\n",
    "    \n",
    "    # === TREND FEATURES ===\n",
    "    df['demand_trend_3m'] = df[target_col].shift(1) - df[target_col].shift(3)\n",
    "    df['demand_trend_6m'] = df[target_col].shift(1) - df[target_col].shift(6)\n",
    "    df['demand_pct_change_1m'] = df[target_col].pct_change(1)\n",
    "    df['demand_pct_change_3m'] = df[target_col].pct_change(3)\n",
    "    \n",
    "    # === SEASONAL FEATURES ===\n",
    "    df['month_of_year'] = df['month'].dt.month\n",
    "    df['quarter'] = df['month'].dt.quarter\n",
    "    df['is_q4'] = (df['quarter'] == 4).astype(int)  # Fiscal year end\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month_of_year'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month_of_year'] / 12)\n",
    "    \n",
    "    # === RISK SIGNAL INTERACTIONS ===\n",
    "    if 'geo_risk_index' in df.columns and 'tariff_risk_index' in df.columns:\n",
    "        df['combined_risk_interaction'] = df['geo_risk_index'] * df['tariff_risk_index']\n",
    "        df['geo_risk_lag_1m'] = df['geo_risk_index'].shift(1)\n",
    "        df['tariff_risk_lag_1m'] = df['tariff_risk_index'].shift(1)\n",
    "    \n",
    "    if 'commodity_cost_pressure' in df.columns:\n",
    "        df['commodity_lag_1m'] = df['commodity_cost_pressure'].shift(1)\n",
    "        df['commodity_trend_3m'] = df['commodity_cost_pressure'].shift(1) - df['commodity_cost_pressure'].shift(3)\n",
    "    \n",
    "    if 'weather_disruption_index' in df.columns:\n",
    "        df['weather_lag_1m'] = df['weather_disruption_index'].shift(1)\n",
    "    \n",
    "    # === TIME FEATURES ===\n",
    "    df['months_since_start'] = (df['month'] - df['month'].min()).dt.days / 30.44\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Engineer features\n",
    "df_engineered = engineer_features(demand_df)\n",
    "\n",
    "# Drop rows with NaN (due to lag/rolling features)\n",
    "df_clean = df_engineered.dropna()\n",
    "\n",
    "print(f\"\u2713 Feature engineering complete\")\n",
    "print(f\"  Original features: {len(demand_df.columns)}\")\n",
    "print(f\"  Engineered features: {len(df_engineered.columns)}\")\n",
    "print(f\"  Clean records: {len(df_clean)} (after removing NaN)\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Features and Split Data\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define feature columns (exclude target, date, and identifiers)\n",
    "exclude_cols = ['month', 'total_obligations_usd', 'prime_obligations_usd', 'subaward_obligations_usd']\n",
    "feature_cols = [col for col in df_clean.columns if col not in exclude_cols and df_clean[col].dtype in ['float64', 'int64']]\n",
    "\n",
    "print(f\"\u2713 Selected {len(feature_cols)} features:\")\n",
    "for col in sorted(feature_cols):\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "# Prepare X and y\n",
    "X = df_clean[feature_cols].values\n",
    "y = df_clean['total_obligations_usd'].values\n",
    "dates = df_clean['month'].values\n",
    "\n",
    "# Time series split (train on past, test on recent)\n",
    "train_size = len(X) - TEST_SIZE\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "dates_train, dates_test = dates[:train_size], dates[train_size:]\n",
    "\n",
    "print(f\"\\n\u2713 Data split:\")\n",
    "print(f\"  Training: {len(X_train)} months ({dates_train[0]} to {dates_train[-1]})\")\n",
    "print(f\"  Testing: {len(X_test)} months ({dates_test[0]} to {dates_test[-1]})\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Random Forest Model\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Start MLflow run\n",
    "mlflow.set_experiment(\"/SupplyChain/RandomForestForecasting\")\n",
    "\n",
    "with mlflow.start_run(run_name=f\"RandomForest_{datetime.now().strftime('%Y%m%d_%H%M')}\"):\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"n_estimators\", N_ESTIMATORS)\n",
    "    mlflow.log_param(\"max_depth\", MAX_DEPTH)\n",
    "    mlflow.log_param(\"min_samples_split\", MIN_SAMPLES_SPLIT)\n",
    "    mlflow.log_param(\"forecast_horizon\", FORECAST_HORIZON)\n",
    "    mlflow.log_param(\"test_size\", TEST_SIZE)\n",
    "    mlflow.log_param(\"n_features\", len(feature_cols))\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Training Random Forest model...\")\n",
    "    rf_model = RandomForestRegressor(\n",
    "        n_estimators=N_ESTIMATORS,\n",
    "        max_depth=MAX_DEPTH,\n",
    "        min_samples_split=MIN_SAMPLES_SPLIT,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    rf_model.fit(X_train, y_train)\n",
    "    print(\"\u2713 Model trained\")\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = rf_model.predict(X_train)\n",
    "    y_test_pred = rf_model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    train_mape = mean_absolute_percentage_error(y_train, y_train_pred) * 100\n",
    "    test_mape = mean_absolute_percentage_error(y_test, y_test_pred) * 100\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"train_mae\", train_mae)\n",
    "    mlflow.log_metric(\"test_mae\", test_mae)\n",
    "    mlflow.log_metric(\"train_rmse\", train_rmse)\n",
    "    mlflow.log_metric(\"test_rmse\", test_rmse)\n",
    "    mlflow.log_metric(\"train_mape\", train_mape)\n",
    "    mlflow.log_metric(\"test_mape\", test_mape)\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(rf_model, \"random_forest_model\")\n",
    "    \n",
    "    print(\"\\n=== MODEL PERFORMANCE ===\")\n",
    "    print(f\"Training Set:\")\n",
    "    print(f\"  MAE:  ${train_mae:,.0f}\")\n",
    "    print(f\"  RMSE: ${train_rmse:,.0f}\")\n",
    "    print(f\"  MAPE: {train_mape:.2f}%\")\n",
    "    print(f\"\\nTest Set:\")\n",
    "    print(f\"  MAE:  ${test_mae:,.0f}\")\n",
    "    print(f\"  RMSE: ${test_rmse:,.0f}\")\n",
    "    print(f\"  MAPE: {test_mape:.2f}%\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Get feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\n=== TOP 15 FEATURE IMPORTANCES ===\")\n",
    "for idx, row in feature_importance.head(15).iterrows():\n",
    "    print(f\"{row['feature']:40s} {row['importance']:.4f}\")\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_n = 20\n",
    "sns.barplot(data=feature_importance.head(top_n), x='importance', y='feature', palette='viridis')\n",
    "plt.title(f'Top {top_n} Feature Importances - Random Forest Demand Forecasting', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Importance Score', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plot\n",
    "importance_plot_path = \"/tmp/rf_feature_importance.png\"\n",
    "plt.savefig(importance_plot_path, dpi=150, bbox_inches='tight')\n",
    "mlflow.log_artifact(importance_plot_path)\n",
    "print(f\"\\n\u2713 Feature importance plot saved\")\n",
    "\n",
    "# Save to Delta table\n",
    "feature_importance['model_type'] = 'RandomForest'\n",
    "feature_importance['training_date'] = datetime.now()\n",
    "feature_importance['rank'] = range(1, len(feature_importance) + 1)\n",
    "\n",
    "spark.createDataFrame(feature_importance) \\\n",
    "    .write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(FEATURE_IMPORTANCE_TABLE)\n",
    "\n",
    "print(f\"\u2713 Feature importance saved to {FEATURE_IMPORTANCE_TABLE}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP Values for Explainability\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Calculating SHAP values (this may take a few minutes)...\")\n",
    "\n",
    "# Use TreeExplainer for Random Forest\n",
    "explainer = shap.TreeExplainer(rf_model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Summary plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values, X_test, feature_names=feature_cols, show=False)\n",
    "plt.tight_layout()\n",
    "\n",
    "shap_plot_path = \"/tmp/rf_shap_summary.png\"\n",
    "plt.savefig(shap_plot_path, dpi=150, bbox_inches='tight')\n",
    "mlflow.log_artifact(shap_plot_path)\n",
    "print(\"\u2713 SHAP summary plot saved\")\n",
    "\n",
    "# Feature importance from SHAP\n",
    "shap_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'shap_importance': np.abs(shap_values).mean(axis=0)\n",
    "}).sort_values('shap_importance', ascending=False)\n",
    "\n",
    "print(\"\\n=== TOP 10 SHAP FEATURE IMPORTANCES ===\")\n",
    "for idx, row in shap_importance.head(10).iterrows():\n",
    "    print(f\"{row['feature']:40s} {row['shap_importance']:.2f}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Future Forecasts\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def forecast_future(model, df_historical, feature_cols, n_months=12):\n",
    "    \"\"\"\n",
    "    Generate future forecasts by iteratively predicting and updating features.\n",
    "    \"\"\"\n",
    "    df_forecast = df_historical.copy()\n",
    "    last_date = df_forecast['month'].max()\n",
    "    \n",
    "    forecasts = []\n",
    "    \n",
    "    for i in range(1, n_months + 1):\n",
    "        # Create next month\n",
    "        next_month = last_date + pd.DateOffset(months=i)\n",
    "        \n",
    "        # Re-engineer features with updated data\n",
    "        df_temp = engineer_features(df_forecast)\n",
    "        df_temp = df_temp.dropna()\n",
    "        \n",
    "        # Get features for last row\n",
    "        if len(df_temp) > 0:\n",
    "            X_next = df_temp[feature_cols].iloc[-1:].values\n",
    "            \n",
    "            # Predict\n",
    "            y_pred = model.predict(X_next)[0]\n",
    "            \n",
    "            # Create forecast row\n",
    "            forecast_row = df_forecast.iloc[-1:].copy()\n",
    "            forecast_row['month'] = next_month\n",
    "            forecast_row['total_obligations_usd'] = y_pred\n",
    "            \n",
    "            # Append to historical data for next iteration\n",
    "            df_forecast = pd.concat([df_forecast, forecast_row], ignore_index=True)\n",
    "            \n",
    "            forecasts.append({\n",
    "                'month': next_month,\n",
    "                'forecast_demand_usd': y_pred\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(forecasts)\n",
    "\n",
    "# Generate forecasts\n",
    "print(f\"Generating {FORECAST_HORIZON}-month forecast...\")\n",
    "future_forecasts = forecast_future(rf_model, df_clean, feature_cols, n_months=FORECAST_HORIZON)\n",
    "\n",
    "print(f\"\u2713 Generated {len(future_forecasts)} months of forecasts\")\n",
    "print(f\"  Forecast range: {future_forecasts['month'].min()} to {future_forecasts['month'].max()}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Prediction Intervals\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Use quantile regression forests approach (estimate from tree predictions)\n",
    "def calculate_prediction_intervals(model, X, confidence=0.95):\n",
    "    \"\"\"\n",
    "    Calculate prediction intervals using individual tree predictions.\n",
    "    \"\"\"\n",
    "    # Get predictions from all trees\n",
    "    tree_predictions = np.array([tree.predict(X) for tree in model.estimators_])\n",
    "    \n",
    "    # Calculate percentiles\n",
    "    lower_percentile = (1 - confidence) / 2 * 100\n",
    "    upper_percentile = (1 + confidence) / 2 * 100\n",
    "    \n",
    "    lower_bound = np.percentile(tree_predictions, lower_percentile, axis=0)\n",
    "    upper_bound = np.percentile(tree_predictions, upper_percentile, axis=0)\n",
    "    \n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "# Calculate intervals for test set\n",
    "y_test_lower, y_test_upper = calculate_prediction_intervals(rf_model, X_test)\n",
    "\n",
    "# Add to test predictions\n",
    "test_results = pd.DataFrame({\n",
    "    'month': dates_test,\n",
    "    'actual_demand_usd': y_test,\n",
    "    'forecast_demand_usd': y_test_pred,\n",
    "    'forecast_lower': y_test_lower,\n",
    "    'forecast_upper': y_test_upper,\n",
    "    'is_actual': True\n",
    "})\n",
    "\n",
    "# Add future forecasts (no actuals, no intervals for simplicity)\n",
    "future_forecasts['forecast_lower'] = future_forecasts['forecast_demand_usd'] * 0.85\n",
    "future_forecasts['forecast_upper'] = future_forecasts['forecast_demand_usd'] * 1.15\n",
    "future_forecasts['actual_demand_usd'] = None\n",
    "future_forecasts['is_actual'] = False\n",
    "\n",
    "# Combine\n",
    "all_forecasts = pd.concat([test_results, future_forecasts], ignore_index=True)\n",
    "\n",
    "print(f\"\u2713 Prediction intervals calculated\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Forecasts\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot actual vs forecast\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Historical actual\n",
    "plt.plot(dates_train, y_train, 'o-', color='blue', label='Training Data', alpha=0.6, markersize=4)\n",
    "\n",
    "# Test actual vs predicted\n",
    "plt.plot(dates_test, y_test, 'o-', color='green', label='Test Actual', markersize=6)\n",
    "plt.plot(dates_test, y_test_pred, 's-', color='orange', label='Test Predicted', markersize=6)\n",
    "plt.fill_between(dates_test, y_test_lower, y_test_upper, color='orange', alpha=0.2, label='95% Prediction Interval')\n",
    "\n",
    "# Future forecast\n",
    "future_dates = future_forecasts['month'].values\n",
    "future_preds = future_forecasts['forecast_demand_usd'].values\n",
    "future_lower = future_forecasts['forecast_lower'].values\n",
    "future_upper = future_forecasts['forecast_upper'].values\n",
    "\n",
    "plt.plot(future_dates, future_preds, 's--', color='red', label='Future Forecast', markersize=6)\n",
    "plt.fill_between(future_dates, future_lower, future_upper, color='red', alpha=0.2)\n",
    "\n",
    "plt.title('Random Forest Demand Forecast - Oshkosh Defense Contracts', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Month', fontsize=12)\n",
    "plt.ylabel('Demand (USD)', fontsize=12)\n",
    "plt.legend(loc='best', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "forecast_plot_path = \"/tmp/rf_forecast_plot.png\"\n",
    "plt.savefig(forecast_plot_path, dpi=150, bbox_inches='tight')\n",
    "mlflow.log_artifact(forecast_plot_path)\n",
    "print(\"\u2713 Forecast plot saved\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Forecasts to Delta\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save to Delta table\n",
    "spark.createDataFrame(all_forecasts) \\\n",
    "    .write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(FORECAST_OUTPUT_TABLE)\n",
    "\n",
    "print(f\"\u2713 Forecasts saved to {FORECAST_OUTPUT_TABLE}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RANDOM FOREST FORECASTING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model: Random Forest Regressor\")\n",
    "print(f\"  - Estimators: {N_ESTIMATORS}\")\n",
    "print(f\"  - Max Depth: {MAX_DEPTH}\")\n",
    "print(f\"  - Features: {len(feature_cols)}\")\n",
    "print(f\"\\nPerformance (Test Set):\")\n",
    "print(f\"  - MAE:  ${test_mae:,.0f}\")\n",
    "print(f\"  - RMSE: ${test_rmse:,.0f}\")\n",
    "print(f\"  - MAPE: {test_mape:.2f}%\")\n",
    "print(f\"\\nTop 5 Most Important Features:\")\n",
    "for idx, row in feature_importance.head(5).iterrows():\n",
    "    print(f\"  {idx+1}. {row['feature']}: {row['importance']:.4f}\")\n",
    "print(f\"\\nForecasts Generated: {len(future_forecasts)} months\")\n",
    "print(f\"  Range: {future_forecasts['month'].min().strftime('%Y-%m')} to {future_forecasts['month'].max().strftime('%Y-%m')}\")\n",
    "print(f\"\\nOutputs:\")\n",
    "print(f\"  - {FORECAST_OUTPUT_TABLE}\")\n",
    "print(f\"  - {FEATURE_IMPORTANCE_TABLE}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Display sample forecasts\n",
    "display(all_forecasts.tail(15))\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}